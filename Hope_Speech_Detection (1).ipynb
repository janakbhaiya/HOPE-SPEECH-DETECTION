{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.22.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\welcome\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\welcome\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\welcome\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.22.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\welcome\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.2)\n",
            "Requirement already satisfied: numpy>=1.15 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (1.22.3)\n",
            "Requirement already satisfied: pandas>=0.23 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (3.5.1)\n",
            "Requirement already satisfied: scipy>=1.0 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (1.8.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.31.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\welcome\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.4.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\welcome\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\welcome\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rn in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\welcome\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7)\n",
            "Requirement already satisfied: tqdm in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.63.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.3.15)\n",
            "Requirement already satisfied: click in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\welcome\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\welcome\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install seaborn\n",
        "%pip install rn\n",
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.8.0)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\welcome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.22.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\welcome\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c2BAcKpIduqI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BkdCwbrSdwaH"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text;label;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What do you mean by the word sniped?;Non_hope_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love this video!! I’m bisexual and it’s just...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ya the irony but then i don't want to come off...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A PERSON'S CHARACTER MATTERS. PERIOD!!;Non_hop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Blaster of Gasters;Non_hope_speech;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2841</th>\n",
              "      <td>+Ashrenneemakeup I think it's all a deliberate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2842</th>\n",
              "      <td>Sheriff David Clarke. This guy is amazing.;Non...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2843</th>\n",
              "      <td>Abandorn Hope Situation;Non_hope_speech;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2844</th>\n",
              "      <td>Sheriff Clarke you are a person of such strong...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2845</th>\n",
              "      <td>Sanders has no room to talk. If there's one pe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2846 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            text;label;\n",
              "0     What do you mean by the word sniped?;Non_hope_...\n",
              "1     I love this video!! I’m bisexual and it’s just...\n",
              "2     ya the irony but then i don't want to come off...\n",
              "3     A PERSON'S CHARACTER MATTERS. PERIOD!!;Non_hop...\n",
              "4                  @Blaster of Gasters;Non_hope_speech;\n",
              "...                                                 ...\n",
              "2841  +Ashrenneemakeup I think it's all a deliberate...\n",
              "2842  Sheriff David Clarke. This guy is amazing.;Non...\n",
              "2843           Abandorn Hope Situation;Non_hope_speech;\n",
              "2844  Sheriff Clarke you are a person of such strong...\n",
              "2845  Sanders has no room to talk. If there's one pe...\n",
              "\n",
              "[2846 rows x 1 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#reading datasets\n",
        "#valid_data = pd.read_csv(\"Hope_ENG_dev.csv\")\n",
        "#train_data = pd.read_csv(\"Hope_ENG_train.csv\")\n",
        "testing_valid = pd.read_csv(r\"C:\\Users\\welcome\\Desktop\\english_hope_test.csv\")\n",
        "testing_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fTx1X-YneDMs"
      },
      "outputs": [],
      "source": [
        "#adding column labels\n",
        "valid_data.columns =['text', 'label']\n",
        "train_data.columns =['text', 'label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HU8tTrTPXYG3",
        "outputId": "7d465242-c3e4-4535-97e4-42e5f847e36e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@Champions Again He got killed for using false...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It's not that all lives don't matter</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Is it really that difficult to understand? Bla...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Whenever we say black isn't that racists?  Why...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ros The Boss u don’t know that she’s actually ...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text            label\n",
              "0  @Champions Again He got killed for using false...  Non_hope_speech\n",
              "1               It's not that all lives don't matter  Non_hope_speech\n",
              "2  Is it really that difficult to understand? Bla...  Non_hope_speech\n",
              "3  Whenever we say black isn't that racists?  Why...  Non_hope_speech\n",
              "4  Ros The Boss u don’t know that she’s actually ...  Non_hope_speech"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_data.head()\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SJXcyzlXbHc",
        "outputId": "fb99b4e6-3468-4e3d-d81d-57989b3600bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. of training examples: 18191\n",
            "No. of testing examples: 4548\n"
          ]
        }
      ],
      "source": [
        "training_data = train_data.sample(frac=0.8, random_state=25)\n",
        "testing_data = train_data.drop(training_data.index)\n",
        "\n",
        "print(f\"No. of training examples: {training_data.shape[0]}\")\n",
        "print(f\"No. of testing examples: {testing_data.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPhbj9MxeEcj",
        "outputId": "0cb1a083-35c4-4f7a-f8b4-bd988771f8f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                   text            label\n",
            "0     @Generation X Counting money that she been giv...  Non_hope_speech\n",
            "1     @Paola Hernandez i never said to be intolerant...  Non_hope_speech\n",
            "2     @Firstlast300 Wow An opinion is that I don't l...  Non_hope_speech\n",
            "3     WOW!!!!!!!That was so so inspiring and incredi...      Hope_speech\n",
            "4     @FALC0n  Yea sorry I know Asian is an ethnicit...  Non_hope_speech\n",
            "...                                                 ...              ...\n",
            "2835                             Such fake sentiment. .  Non_hope_speech\n",
            "2836  @A G black lives arent undervalued compared to...  Non_hope_speech\n",
            "2837  People who pulled it down can and will be arre...  Non_hope_speech\n",
            "2838  @Aaron Castellanos It will be a two hour movie...  Non_hope_speech\n",
            "2839  why is there no footage of the riots and the v...      Hope_speech\n",
            "\n",
            "[2840 rows x 2 columns]\n",
            "                                                    text            label\n",
            "8387   The amount of homophobia in this comment secti...  Non_hope_speech\n",
            "19928                           Watching in 2017 anyone?  Non_hope_speech\n",
            "17586  Isobel Mclean Your Jesus Christ has failed to ...  Non_hope_speech\n",
            "19099                              You missed some clips  Non_hope_speech\n",
            "14554                     I bet you secretly like to eat  Non_hope_speech\n",
            "...                                                  ...              ...\n",
            "14550  Let's not say White lives matter..we are bad p...  Non_hope_speech\n",
            "5711                        The gap in her teeth is back  Non_hope_speech\n",
            "84     I'm glad Madonna knows there is a market for O...  Non_hope_speech\n",
            "20307  @Helium Forget what? The fact that people are ...  Non_hope_speech\n",
            "1227   Hate me for saying All Lives Matter. you know ...  Non_hope_speech\n",
            "\n",
            "[18191 rows x 2 columns]\n",
            "                                                    text            label\n",
            "7            There were a lot of Samoans in my Army unit  Non_hope_speech\n",
            "14     @Tea Just Tea all lives are equal is the same ...  Non_hope_speech\n",
            "32     All I’m gunna say is I was Presbyterian and “d...  Non_hope_speech\n",
            "35     Right...that's what he said. You have it all r...  Non_hope_speech\n",
            "62     Chloe Faye I mean they are there to hear his s...  Non_hope_speech\n",
            "...                                                  ...              ...\n",
            "22718  Same!!! I feel like she had her guard up about...  Non_hope_speech\n",
            "22722         Hes so good with kids. So patient and kind  Non_hope_speech\n",
            "22724  Wow. White lives don't matter. Looks like they...  Non_hope_speech\n",
            "22728  Circulqr your literally contradicting yourself...  Non_hope_speech\n",
            "22736                          God says her life matters  Non_hope_speech\n",
            "\n",
            "[4548 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "#printing our datasets\n",
        "print(valid_data)\n",
        "print(training_data)\n",
        "print(testing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x9Oxe-K-eQ2W"
      },
      "outputs": [],
      "source": [
        "#encoding our data to 0 and 1\n",
        "training_data['enc_label'] = training_data['label'].replace({'Non_hope_speech':0, 'Hope_speech':1})\n",
        "valid_data['enc_label'] = valid_data['label'].replace({'Non_hope_speech':0, 'Hope_speech':1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3EeduYQeS3E",
        "outputId": "e45e59ba-76b6-4c93-df82-4272524c4bc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     C:\\Users\\welcome\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "#removing usernames\n",
        "import re\n",
        "\n",
        "def remove_usernames_links(tweet):\n",
        "    tweet = re.sub('@[^\\s]+','',tweet)\n",
        "    tweet = re.sub('http[^\\s]+','',tweet)\n",
        "    return tweet\n",
        "training_data['text'] = training_data['text'].apply(remove_usernames_links)\n",
        "\n",
        "#cleaning text\n",
        "\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() \n",
        "\n",
        "def preprocess(sentence):\n",
        "    sentence=str(sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence=sentence.replace('{html}',\"\") \n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', sentence)\n",
        "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(rem_num)  \n",
        "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
        "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "   \n",
        "\n",
        "training_data['cleanText']= training_data['text'].map(lambda s:preprocess(s)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "v0u_U6x9fwqt",
        "outputId": "abb5b43d-5a7f-4ad1-8d33-d948428a683c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>enc_label</th>\n",
              "      <th>cleanText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8387</th>\n",
              "      <td>The amount of homophobia in this comment secti...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>0</td>\n",
              "      <td>amount homophobia comment section fucking insane</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19928</th>\n",
              "      <td>Watching in 2017 anyone?</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>0</td>\n",
              "      <td>watching anyone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17586</th>\n",
              "      <td>Isobel Mclean Your Jesus Christ has failed to ...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>0</td>\n",
              "      <td>isobel mclean jesus christ failed prevent many...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19099</th>\n",
              "      <td>You missed some clips</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>0</td>\n",
              "      <td>missed clips</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14554</th>\n",
              "      <td>I bet you secretly like to eat</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>0</td>\n",
              "      <td>bet secretly like eat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14550</th>\n",
              "      <td>Let's not say White lives matter..we are bad p...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>0</td>\n",
              "      <td>let say white lives matter bad people saying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5711</th>\n",
              "      <td>The gap in her teeth is back</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>0</td>\n",
              "      <td>gap teeth back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>I'm glad Madonna knows there is a market for O...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>0</td>\n",
              "      <td>glad madonna knows market originality brillian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20307</th>\n",
              "      <td>Forget what? The fact that people are being s...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>0</td>\n",
              "      <td>forget fact people senselessly killed police h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1227</th>\n",
              "      <td>Hate me for saying All Lives Matter. you know ...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>0</td>\n",
              "      <td>hate saying lives matter know racist smfh</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18191 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text            label  \\\n",
              "8387   The amount of homophobia in this comment secti...  Non_hope_speech   \n",
              "19928                           Watching in 2017 anyone?  Non_hope_speech   \n",
              "17586  Isobel Mclean Your Jesus Christ has failed to ...  Non_hope_speech   \n",
              "19099                              You missed some clips  Non_hope_speech   \n",
              "14554                     I bet you secretly like to eat  Non_hope_speech   \n",
              "...                                                  ...              ...   \n",
              "14550  Let's not say White lives matter..we are bad p...  Non_hope_speech   \n",
              "5711                        The gap in her teeth is back  Non_hope_speech   \n",
              "84     I'm glad Madonna knows there is a market for O...  Non_hope_speech   \n",
              "20307   Forget what? The fact that people are being s...  Non_hope_speech   \n",
              "1227   Hate me for saying All Lives Matter. you know ...  Non_hope_speech   \n",
              "\n",
              "       enc_label                                          cleanText  \n",
              "8387           0   amount homophobia comment section fucking insane  \n",
              "19928          0                                    watching anyone  \n",
              "17586          0  isobel mclean jesus christ failed prevent many...  \n",
              "19099          0                                       missed clips  \n",
              "14554          0                              bet secretly like eat  \n",
              "...          ...                                                ...  \n",
              "14550          0       let say white lives matter bad people saying  \n",
              "5711           0                                     gap teeth back  \n",
              "84             0  glad madonna knows market originality brillian...  \n",
              "20307          0  forget fact people senselessly killed police h...  \n",
              "1227           0          hate saying lives matter know racist smfh  \n",
              "\n",
              "[18191 rows x 4 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "zycBJbyWgm58",
        "outputId": "8c0d1f46-92e7-4e0e-fea2-5e6865ff9b51"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>enc_label</th>\n",
              "      <th>cleanText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8387</th>\n",
              "      <td>0</td>\n",
              "      <td>amount homophobia comment section fucking insane</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19928</th>\n",
              "      <td>0</td>\n",
              "      <td>watching anyone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17586</th>\n",
              "      <td>0</td>\n",
              "      <td>isobel mclean jesus christ failed prevent many...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19099</th>\n",
              "      <td>0</td>\n",
              "      <td>missed clips</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14554</th>\n",
              "      <td>0</td>\n",
              "      <td>bet secretly like eat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14550</th>\n",
              "      <td>0</td>\n",
              "      <td>let say white lives matter bad people saying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5711</th>\n",
              "      <td>0</td>\n",
              "      <td>gap teeth back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0</td>\n",
              "      <td>glad madonna knows market originality brillian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20307</th>\n",
              "      <td>0</td>\n",
              "      <td>forget fact people senselessly killed police h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1227</th>\n",
              "      <td>0</td>\n",
              "      <td>hate saying lives matter know racist smfh</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18191 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       enc_label                                          cleanText\n",
              "8387           0   amount homophobia comment section fucking insane\n",
              "19928          0                                    watching anyone\n",
              "17586          0  isobel mclean jesus christ failed prevent many...\n",
              "19099          0                                       missed clips\n",
              "14554          0                              bet secretly like eat\n",
              "...          ...                                                ...\n",
              "14550          0       let say white lives matter bad people saying\n",
              "5711           0                                     gap teeth back\n",
              "84             0  glad madonna knows market originality brillian...\n",
              "20307          0  forget fact people senselessly killed police h...\n",
              "1227           0          hate saying lives matter know racist smfh\n",
              "\n",
              "[18191 rows x 2 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = training_data\n",
        "df = df.drop(['text', 'label'], axis = 1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hQD6msbNgKII"
      },
      "outputs": [],
      "source": [
        "X = df.cleanText\n",
        "y = df.enc_label\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4_7UtPOFhM38"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "Xtrain_vects = vectorizer.fit_transform(X_train)\n",
        "Xtest_vects = vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "L5Xwd-jmivKm"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(random_state=0).fit(Xtrain_vects, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iLyae2_xYoQu"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(random_state=48).fit(Xtrain_vects, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIiSgcXGZUSz",
        "outputId": "b37f4e4a-e889-49c5-f517-a0ce2c1437c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on Training Data : 0.9960830126443101\n",
            "Accuracy ob Testing Data : 0.9211321791701017\n",
            "The normal f1score : [0.95825455 0.28784119] \n",
            "The weighted f1score : 0.8993009116529473 \n",
            "The macro f1score : 0.6230478682607714 \n",
            "The micro f1score : 0.9211321791701017\n",
            "The microrecall score : 0.9211321791701017 \n",
            "The macrorecall score : 0.6230478682607714 \n",
            "The weightedrecall score : 0.9211321791701017 \n",
            "The normalrecall score : [0.99246761 0.18125   ]\n",
            "The micro precision score : 0.9211321791701017 \n",
            "The macro precision score : 0.8125584452545842 \n",
            "The weighted precision score : 0.9063138809046379 \n",
            "The normal precision score : [0.92632171 0.69879518]\n"
          ]
        }
      ],
      "source": [
        "score = clf.score(Xtrain_vects, y_train)\n",
        "score2 = clf.score(Xtest_vects, y_test)\n",
        "print(\"Accuracy on Training Data :\",score)\n",
        "print(\"Accuracy ob Testing Data :\",score2)\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "normalscore=f1_score(y_test, clf.predict(Xtest_vects), average = None)\n",
        "weightedscore =f1_score(y_test, clf.predict(Xtest_vects), average = 'weighted')\n",
        "macroscore= f1_score(y_test, clf.predict(Xtest_vects), average = 'macro')\n",
        "microscore= f1_score(y_test, clf.predict(Xtest_vects), average = 'micro')\n",
        "microrecall = recall_score(y_test, clf.predict(Xtest_vects), average='micro')\n",
        "macrorecall = recall_score(y_test, clf.predict(Xtest_vects), average='macro')\n",
        "normalrecall = recall_score(y_test, clf.predict(Xtest_vects), average=None)\n",
        "weightedrecall = recall_score(y_test, clf.predict(Xtest_vects), average='weighted')\n",
        "macroprecisionscore= precision_score(y_test, clf.predict(Xtest_vects), average='macro')\n",
        "weightedprecisionscore= precision_score(y_test, clf.predict(Xtest_vects), average='weighted')\n",
        "microprecisionscore= precision_score(y_test, clf.predict(Xtest_vects), average='micro')\n",
        "normalprecisionscore= precision_score(y_test, clf.predict(Xtest_vects), average=None)\n",
        "print('The normal f1score :',normalscore,'\\nThe weighted f1score :',weightedscore,'\\nThe macro f1score :',macroscore,'\\nThe micro f1score :',microscore)\n",
        "print('The microrecall score :',microrecall,'\\nThe macrorecall score :',macroscore,'\\nThe weightedrecall score :',weightedrecall,'\\nThe normalrecall score :',normalrecall)\n",
        "print('The micro precision score :',microprecisionscore,'\\nThe macro precision score :',macroprecisionscore,'\\nThe weighted precision score :',weightedprecisionscore,'\\nThe normal precision score :',normalprecisionscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-2WRbBmZWTT",
        "outputId": "077b56b8-f6b0-4186-dd33-425514c823e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "95.86075959072059"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy = ((score + score2) / 2) *100\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Hope Speech Detection",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
